from typing import Dict, List
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize
import pandas as pd
from scipy.sparse import hstack

def _build_docs_from_occurrences(occurrences: Dict[str, List[dict]]) -> Dict[str, str]:
    docs = {}
    for attr, occs in occurrences.items():
        parts = [attr]
        for o in occs:
            parts.append(f"{o['value']} :: {o['context']}")
        docs[attr] = " | ".join(parts) if parts else attr
    return docs

def compute_similarity_matrix(occurrences: Dict[str, List[dict]]) -> pd.DataFrame:
    docs = _build_docs_from_occurrences(occurrences)
    attrs = list(docs.keys())
    corpus = [docs[a] for a in attrs]

    vectorizer_char = TfidfVectorizer(analyzer="char_wb", ngram_range=(3,5), min_df=1)
    X_char = vectorizer_char.fit_transform(corpus)

    vectorizer_word = TfidfVectorizer(analyzer="word", ngram_range=(1,2), min_df=1)
    X_word = vectorizer_word.fit_transform(corpus)

    X = hstack([X_char, X_word])

    # Reduce to dense semantic space
    k = min(64, max(2, X.shape[1]-1))
    svd = TruncatedSVD(n_components=k, random_state=42)
    Z = svd.fit_transform(X)
    Z = normalize(Z)

    # cosine similarity
    S = Z @ Z.T
    S = (S + S.T) / 2.0

    import numpy as np
    import pandas as pd
    df = pd.DataFrame(S, index=attrs, columns=attrs)
    return df
